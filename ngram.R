twitter <- readLines(con <- file("C:/Users/user/Documents/DS capstone/Coursera-SwiftKey/en_US/en_US.twitter.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)
blog <- readLines(con <- file("C:/Users/user/Documents/DS capstone/Coursera-SwiftKey/en_US/en_US.blogs.txt"), encoding = "UTF-8", skipNul = TRUE)
close(con)
con <- file("C:/Users/user/Documents/DS capstone/Coursera-SwiftKey/en_US/en_US.news.txt", "rb")
news <- readLines(con, encoding = "UTF-8")
close(con)
length(twitter)
length(blog)
length(news)
format(object.size(twitter), units = "Mb")
format(object.size(blog), units = "Mb")
format(object.size(news), units = "Mb")
set.seed(123)
nsample <- sample(news, size = length(news)*0.01)
tsample <- sample(twitter, size = length(twitter)*0.01)
bsample <- sample(blog, size = length(blog)*0.01)
samp <- c(nsample, tsample, bsample)
rm(news, twitter, blog, con)
gc()
library(tm)
mycorp <- VCorpus(VectorSource(samp), readerControl = list(language = "en"))
mycorp <- tm_map(mycorp, content_transformer(tolower))
mycorp <- tm_map(mycorp, removeNumbers)
mycorp <- tm_map(mycorp, stripWhitespace)
remapostrophe <- function(x) { gsub("'s", "", x)}
remhyphen <- function(x) { gsub("-", " ", x)}
mycorp <- tm_map(mycorp, content_transformer(remapostrophe))
mycorp <- tm_map(mycorp, content_transformer(remhyphen))
rm_punc <- function(x) { gsub("[^'[:^punct:]]", "",x, perl = T)}
mycorp <- tm_map(mycorp, content_transformer(rm_punc))
profane <- c("nigga", "ass", "pussy", "slut", "dick", "asshole")
mycorp <- tm_map(mycorp, removeWords, profane)
mycorp  <- tm_map(mycorp, removeWords, stopwords("english"))
library(RWeka)
library(slam)
library(ggplot2)
unigram <- function(x) NGramTokenizer(x, Weka_control(min=1, max=1))
unidtm <- DocumentTermMatrix(mycorp, control = list(tokenize = unigram))
unidtm <- removeSparseTerms(unidtm, 0.99)
freqterm <- findFreqTerms(unidtm)
csum <- slam::col_sums(unidtm[,unidtm$dimnames$Terms %in% freqterm])
wordfreq <- data.frame(word = freqterm, Freq = csum)
wordfreq <- wordfreq[order(wordfreq$Freq, decreasing = T),]
plot1 <- ggplot(wordfreq[1:10,], aes(x = Freq, y= word) ) + geom_bar(stat = "identity")
plot1
bigram <- function(x) NGramTokenizer(x, Weka_control(min=2, max=2))
bidtm <- DocumentTermMatrix(mycorp, control = list(tokenize = bigram))
bidtm <- removeSparseTerms(bidtm, 0.999)
freqterm2 <- findFreqTerms(bidtm)
csum2 <- slam::col_sums(bidtm[,bidtm$dimnames$Terms %in% freqterm2])
wordfreq2 <- data.frame(word = freqterm2, Freq = csum2)
wordfreq2 <- wordfreq2[order(wordfreq2$Freq, decreasing = T),]
plot2 <- ggplot(wordfreq2[1:10,], aes(x = Freq, y= word) ) + geom_bar(stat = "identity")
plot2
trigram <- function(x) NGramTokenizer(x, Weka_control(min=3, max=3))
tridtm <- DocumentTermMatrix(mycorp, control = list(tokenize = trigram))
tridtm <- removeSparseTerms(tridtm, 0.9999)
freqterm3 <- findFreqTerms(tridtm)
csum3 <- slam::col_sums(tridtm[,tridtm$dimnames$Terms %in% freqterm3])
wordfreq3 <- data.frame(word = freqterm3, Freq = csum3)
wordfreq3 <- wordfreq3[order(wordfreq3$Freq, decreasing = T),]
plot3 <- ggplot(wordfreq3[1:10,], aes(x = Freq, y= word) ) + geom_bar(stat = "identity")
plot3